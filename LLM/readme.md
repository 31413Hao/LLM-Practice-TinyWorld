### 1、train_vocab 训练分词器

  网页数据流获取数据集、生成文件路径、使用SentencePiece训练分词器

### 2、tokenizer 

  利用训练好的分词器定义Tokenizer类，实现encoder、decoder

### 3、preprocess 数据预处理

- ​	定义分片处理函数，将分片数据处理成机器能理解的二进制文件
- ​	定义批处理分片数据函数，使用进程池ProcessPoolExecutor()并行处理多个分片数据

### 4、model

一个Decoder-only的transformer

#### 关于为什么现在普遍使用Decoder-only：

首先是理解decoder、encoder，

**初始理解**：

- **Encoder (编码器)** 的作用是将人类数据编码成电脑能看懂的内容，核心任务是**理解数据**。
- **Decoder (解码器)** 的作用是电脑将自己的内容解码给人类看，核心任务是**生成内容**。

**初步结论**：当需求是让计算机**帮助理解数据**时，使用 Encoder；当需求是让计算机**根据自己的数据生成内容**时，使用 Decoder。

#### 关于Decoder-only如何理解语义：

前面已讨论可以简单地认为encoder是计算机用来理解人类数据的，那么decoder-only的模型是如何理解语义的？

- **产生的理解偏差**：

  > “所以生成式大模型可以使用 Decoder-only 是因为它不需要 Encoder 来深入理解文本语义，只需要知道提供的人类文本的语言顺序等信息就行了。”

- **问题剖析**：这个理解的偏差在于，它错误地认为 Decoder-only 模型**放弃了**对语义的深入理解。如果一个模型不理解语义，它就不可能根据问题生成有逻辑、有意义的答案。

- **修正后的核心结论**： Decoder-only 模型**并没有放弃深入理解语义**，而是找到了一种更高效的方式，**将“理解”和“生成”这两个动作无缝地融合在了同一个自回归（auto-regressive）的过程中**。

- **如何做到的？**

  1. **统一的视角**：模型不区分“需要理解的输入”和“需要生成的输出”。它把用户输入的提示（Prompt）看作是**一段已经写好的开头**。
  2. **在“续写”中完成“理解”**：当模型逐词处理这段“开头”（即用户的 Prompt）时，通过其内部的**因果自注意力机制（Causal Self-Attention）**，每个词的向量表示都会不断地吸收它前面所有词的上下文信息。
  3. **理解的体现**：当处理到 Prompt 的最后一个词时，它的向量表示已经是一个高度浓缩的、包含了整个 Prompt **完整语义**的“上下文向量”。**这个过程本身，就是一次极其深入的语义理解**。
  4. **基于理解去生成**：模型利用这个已经富含语义的上下文向量，来预测并生成下一个最可能的词，然后不断重复这个过程。

> 总结：关键在于理解 Decoder-only 架构的巧妙之处——它不是“不需要理解”，而是“在生成准备阶段就完成了理解”。它把理解这个动作，内化到了为生成下一个词做准备的过程中。
>
> decoder-only的问答能力来源于：现有问答数据的直接学习+深层语义关联的推理结果=强大的泛化能力



RMSNorm+RoPE+GQA（分组查询注意力）

GQA的核心思想是：让多组查询头（Query Head）共享同一组键/值头（Key/Value Head）。

n_rep = n_heads / n_kv_heads：重复键值对的数量=查询头数/可用键值对数

### 5、train



